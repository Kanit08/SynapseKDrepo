{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "azurekdsynapsespark"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"azurekdsynapsespark-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'azurekdsynapsespark-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:azurekdsynapsespark.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"azurekdsynapsespark-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://azurekdls.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/empdata')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "Empdata.csv",
						"folderPath": "Input",
						"container": "datafactorykdcontnr"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "emp",
						"type": "String"
					},
					{
						"name": "name",
						"type": "String"
					},
					{
						"name": "dept",
						"type": "String"
					},
					{
						"name": "grade",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azurekdsynapsespark-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('azurekdsynapsespark-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azurekdsynapsespark-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('azurekdsynapsespark-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Arrays')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "96d3081a-f7d6-4d05-a7a3-8fea17edb17a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"data1=[(\"ron\",[1,2]),\r\n",
							"        (\"don\",[3,4]),\r\n",
							"        (\"son\",[5,6])]\r\n",
							"#schema1=[\"name\",\"number\"]\r\n",
							"schema1=StructType(\\\r\n",
							"                    [StructField(\"name\",StringType()),\\\r\n",
							"                    StructField(\"number\",ArrayType(IntegerType()))\\\r\n",
							"                    ])\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.printSchema()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df1=df.withColumn(\"new number\",col('number')[0])\r\n",
							"display(df1)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"data1=[(1,\"ron\",[\"aws\",\"python\"]),\r\n",
							"        (2,\"don\",[\"azure\",\"java\"]),\r\n",
							"        (3,\"son\",[\"gcp\",\".net\"])]\r\n",
							"schema1=[\"id\",\"name\",\"skills\"]\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df1=df.withColumn(\"explodecol\",explode(col('skills')))\r\n",
							"df1.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data2=[(1,\"ron\",\"aws,python\"),\r\n",
							"        (2,\"don\",\"azure,java\"),\r\n",
							"        (3,\"son\",\"gcp,.net\")]\r\n",
							"schema2=[\"id\",\"name\",\"skills\"]\r\n",
							"df4=spark.createDataFrame(data=data2,schema=schema2)\r\n",
							"df4.show()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2=df4.withColumn(\"split\",split('skills',','))\r\n",
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data3=[(1,\"ron\",\"aws\",\"python\"),\r\n",
							"        (2,\"don\",\"azure\",\"java\"),\r\n",
							"        (3,\"son\",\"gcp\",\".net\")]\r\n",
							"schema3=[\"id\",\"name\",\"skill1\",\"skill2\"]\r\n",
							"df5=spark.createDataFrame(data=data3,schema=schema3)\r\n",
							"df5.show()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df6=df5.withColumn(\"Skill\",array(col('skill1'),col('skill2')))\r\n",
							"df6.show()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data7=[(1,\"ron\",[\"aws\",\"python\"]),\r\n",
							"        (2,\"don\",[\"azure\",\"java\"]),\r\n",
							"        (3,\"son\",[\"gcp\",\".net\"])]\r\n",
							"schema7=[\"id\",\"name\",\"skills\"]\r\n",
							"df7=spark.createDataFrame(data=data7,schema=schema7)\r\n",
							"df7.show()"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df8=df7.withColumn(\"Skilljava\",array_contains(col('skills'),'java'))\r\n",
							"df8.show()"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Collect')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9f01dbd5-860d-4802-84a5-86bf28042434"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",None,2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',None,'IT'),(9,\"Ketan\",'M',4000,None)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"list1=df.collect()\n",
							"print(list1)\n",
							"print(list1[1][1])"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/FromJsonfunction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0b31c8a2-1343-499f-8793-1ba0eef4dce3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[('kanit','{\"hair\":\"black\",\"eye\":\"brown\"}')]\n",
							"schema=['name','properties']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"df1=df.withColumn('propmap',from_json(df.properties,MapType(StringType(),StringType())))\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"df2=df1.withColumn('eyes',df1.propmap.eye).show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[('kanit','{\"hair\":\"black\",\"eye\":\"brown\"}')]\n",
							"schema=['name','properties']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"df.show()\n",
							"propschema=StructType([StructField(\"hair\",StringType()),\\\n",
							"                        StructField('eye',StringType())])\n",
							"df1=df.withColumn('propmap',from_json(df.properties,propschema))\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df2=df1.withColumn('propmap2',df1.propmap.hair).show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getjson')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fe0f2acc-234d-4963-9ebc-2e2caed3c2b8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[('kanit','{\"Address\":{\"City\":\"Pune\",\"state\":\"Odisha\"},\"gender\":\"Male\"}\"'),\\\n",
							"       ('Ankit','{\"Address\":{\"City\":\"Delhi\",\"state\":\"Kerala\"},\"gender\":\"Male\"}\"')]\n",
							"schema=['name','properties']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"df.show(truncate=False)\n",
							"\n",
							"df1=df.select('name',get_json_object('properties','$.Address.City').alias('Place'))\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"help(get_json_object)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Global')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e6275fba-f328-41ae-8f3d-efbf0dc6a0de"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%sql\n",
							"SELECT name FROM global_temp.employees"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Groupby')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b3fa58ad-006e-4bf9-a0eb-6854cbaa57b5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",'F',2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\r\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\r\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',4000,'IT'),(9,\"Ketan\",'M',4000,'IT')]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\r\n",
							"df=spark.createDataFrame(data1,schema1)\r\n",
							"df.show()\r\n",
							"df.groupBy(df.department).count().show()\r\n",
							"df.groupBy(df.department).min('salary').show()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()\r\n",
							"df.groupBy(df.department).agg(count('*').alias('countofemp'),\\\r\n",
							"            min('salary').alias('minsal'),\\\r\n",
							"            max('salary').alias('maxsal')).show()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json_tuple')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2375e1f9-1de7-4210-b4b3-e2c559799494"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[('kanit','{\"hair\":\"black\",\"eye\":\"brown\"}'),\\\n",
							"        ('Devyani','{\"hair\":\"Brown\",\"eye\":\"blue\"}')]\n",
							"schema=['name','properties']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"df.show(truncate=False)\n",
							"\n",
							"df1=df.select('name',json_tuple(df.properties,'hair','eye').alias('Hair','eyes'))\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Maptype')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "949324ea-bce6-4077-b8ee-47240da095fc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(\"ron\",{\"id\":\"KDcog\",\"place\":\"Pune\"}),\r\n",
							"        (\"don\",{\"id\":\"devgen\",\"place\":\"Mumbai\"}),\r\n",
							"        (\"son\",{\"id\":\"anieric\",\"place\":\"Blore\"})]\r\n",
							"#schema1=[\"name\",\"properties\"]\r\n",
							"schema2=StructType([\r\n",
							"                    StructField('name',StringType()),\\\r\n",
							"                    StructField('properties',StringType())])\r\n",
							"df1=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2=df1.withColumn(\"resides\",df1.properties['place'])\r\n",
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3=df2.withColumn(\"identity\",df1.properties['id'])\r\n",
							"df3.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(\"ron\",{\"id\":\"KDcog\",\"place\":\"Pune\"}),\r\n",
							"        (\"don\",{\"id\":\"devgen\",\"place\":\"Mumbai\"}),\r\n",
							"        (\"son\",{\"id\":\"anieric\",\"place\":\"Blore\"})]\r\n",
							"schema1=[\"name\",\"properties\"]\r\n",
							"#schema2=StructType([\r\n",
							"#                    StructField('name',StringType()),\\\r\n",
							"#                    StructField('properties',StringType())])\r\n",
							"df1=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2= df1.select('name','properties',explode(df1.properties)).show(truncate=False)\r\n",
							"df3=df1.withColumn('keys',df1.properties).show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 24
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RddtoDF')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3d9a48c7-86bf-4455-804f-b1d156998c69"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",' ',3000)]\n",
							"rdd=spark.sparkContext.parallelize(data1)\n",
							"print(rdd.collect())\n",
							"schema1=[\"id\",\"name\",'gender','salary']\n",
							"df=spark.createDataFrame(rdd,schema=schema1)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"data1=[(\"kanit\",'Das'),(\"Dev\",'Patil'),(\"Ani\",'Katti')]\n",
							"rdd=spark.sparkContext.parallelize(data1)\n",
							"rdd1=rdd.map(lambda x: x + (x[0]+' '+ x[1],))\n",
							"print(rdd1.collect())"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"df=spark.createDataFrame(data1,['fn','ln'])\n",
							"df1=spark.createDataFrame(rdd1,['fn','ln','fullname'])\n",
							"df1.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLtempandglobalviews')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8f439229-3ebe-4185-9e2c-359b0ce1677c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",None,2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',None,'IT'),(9,\"Ketan\",'M',4000,None)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"#df.createOrReplaceTempView('employees')\n",
							"df.createOrReplaceGlobalTempView('employees')\n",
							"\n",
							"df1 = spark.sql(\"SELECT name FROM global_temp.employees\")\n",
							"df1.show()\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT name FROM global_temp.employees"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"session_id = spark.sparkContext.applicationId\n",
							"print(f\"Session ID: {session_id}\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0d4b42d3-67c0-4cba-b2d5-80135657749b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",None,2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',None,'IT'),(9,\"Ketan\",'M',4000,None)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"def converttoupper(df):\n",
							"    return df.withColumn('name',upper(df.name))\n",
							"def doublesalary(df):\n",
							"    return df.withColumn('salary',df.salary*2)\n",
							"\n",
							"df1=df.transform(converttoupper).transform(doublesalary).show()\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UDFUserdefinedfunction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "859d5277-c7f8-4dd6-a25a-e03c210599f3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,200),(2,\"Dev\",'M',2000,400),(3,\"Ani\",'M',3000,400),\\\n",
							"        (4,\"Ankit\",'M',4000,6000),(5,\"Hari\",'M',1000,900),(6,\"meera\",'F',2000,500)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','bonus']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"def totalsal(s,b):\n",
							"    return s+b\n",
							"#totalpay=udf(lambda x,y: totalsal(x,y),IntegerType())\n",
							"df.select('*',totalsal(col('salary'),col('bonus')).alias('totalsalary')).show()\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,200),(2,\"Dev\",'M',2000,400),(3,\"Ani\",'M',3000,400),\\\n",
							"        (4,\"Ankit\",'M',4000,6000),(5,\"Hari\",'M',1000,900),(6,\"meera\",'F',2000,500)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','bonus']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df1=df.createOrReplaceTempView('emps')\n",
							"df.show()\n",
							"def totalsal(s,b):\n",
							"    return s+b\n",
							"spark.udf.register('Totalpayment',f=totalsal,returnType=IntegerType())"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select *, Totalpayment(salary,bonus) as totpay from emps"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ascdesccastlikeduplicate')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f0565013-e84a-4922-a4e6-33f17686b633"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",' ',3000)]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary']\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.sort(df.name.asc()).show()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.filter(df.name.like('k%')).show()\r\n",
							"df.filter(df.name=='Dev').show()\r\n",
							"df.filter((df.name=='Dev')&(df.gender=='F')).show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df1=df.select(df.salary.cast('int'))\r\n",
							"df1.printSchema()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",'M',3000),(3,\"Ani\",'M',3000),\\\r\n",
							"        (4,\"Ankit\",'M',4000)]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary']\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.drop_duplicates().show()\r\n",
							"df.drop_duplicates(['gender']).show()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/createandreadDF')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ac8a5aa5-689e-4fa1-bf85-a9d4593d16b7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"data1 = [(1,\"Kanit\"),(2,\"Dev\")]\r\n",
							"df=spark.createDataFrame(data=data1)\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data2=[{\"name\":'kanit',\"age\":30},\r\n",
							"{\"name\":'dev',\"age\":31}]\r\n",
							"df=spark.createDataFrame(data=data2)\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql.types import *\r\n",
							"schema1=StructType().add(field='emp',data_type=IntegerType())\\\r\n",
							"                    .add(field='name',data_type=StringType())\\\r\n",
							"                    .add(field='dept',data_type=StringType())\\\r\n",
							"                    .add(field='grade',data_type=StringType())\r\n",
							"df = spark.read.load('abfss://kdlsfile@azurekdls.dfs.core.windows.net/Empdata.csv', schema=schema1, format='csv'\r\n",
							"## Ifheaderexistsuncommentlinebelow\r\n",
							", header=True\r\n",
							")\r\n",
							"display(df.limit(10))\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/date_function')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "92227325-31c4-4d01-b1f2-4e02895a299e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"df=spark.range(3)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df1=df.withColumn('date',current_date())\n",
							"df1.show()\n",
							"df2=df1.withColumn('dateformat',date_format('date', 'MM/dd/yyy'))\n",
							"df2.show()\n",
							"df3=df2.withColumn('currdate',to_date(df2.dateformat,'MM/dd/yyyy'))\n",
							"df3.show()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"data1=[('2024-02-26','2024-12-10')]\n",
							"schema=['date1','date2']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df.withColumn('differencedate',datediff(df.date2,df.date1)).show()\n",
							"df.withColumn('monthsgap',months_between(df.date2,df.date1)).show()\n",
							"df.withColumn('addmonths',add_months(df.date2,2)).show()\n",
							"df.withColumn('minusmonths',add_months(df.date2,-5)).show()\n",
							"df.withColumn('adddate',date_add(df.date1,20)).show()\n",
							"df.withColumn('minusdate',date_add(df.date1,-50)).show()\n",
							"df.withColumn('year',year(df.date1)).show()\n",
							"df.withColumn('month',month(df.date1)).show()\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/fillna')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8754cf82-0dd5-4e6d-9b77-a969cf024ffc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",None,2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',None,'IT'),(9,\"Ketan\",'M',4000,None)]\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"df.fillna('unknown').show()\n",
							"df.fillna('unknown',['gender']).show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/flatmap')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "543051d3-af57-4d3a-b672-c90edfd6daa1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"data1=['kanit Das','Dev Patil','Ani Katti']\n",
							"rdd=spark.sparkContext.parallelize(data1)\n",
							"for i in rdd.collect():\n",
							"    print(i)\n",
							"rdd1=rdd.map(lambda x: x.split(' '))\n",
							"#print(rdd1.collect())"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"for j in rdd2.collect():\n",
							"    print(j)\n",
							"rdd2=rdd.flatMap(lambda x: x.split(' '))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/functiontransform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "63ac87d0-7f6c-40a7-8966-6e5ff00ced84"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"data1=[(1,\"ron\",[\"aws\",\"python\"]),\n",
							"        (2,\"don\",[\"azure\",\"java\"]),\n",
							"        (3,\"son\",[\"gcp\",\".net\"])]\n",
							"schema1=[\"id\",\"name\",\"skills\"]\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\n",
							"df.show()\n",
							"#df1=df.select('id','name',transform('skills',lambda x: upper(x)).alias('skill'))\n",
							"#df1.show()\n",
							"\n",
							"def convertoupper(x):\n",
							"    return upper(x)\n",
							"\n",
							"df2=df.select('id','name',transform('skills',convertoupper).alias('skill')).show()\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"help(df.transform)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/joinfunction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "258abcea-5973-4ed0-bd0c-f0f9bb16e32f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"data1=[(1,'kanit',2000,1),(2,'devi',3000,2),(3,'ani',1000,4)]\n",
							"data2=[(1,'IT'),(2,'HR'),(3,'payroll')]\n",
							"schema1=['id','name','salary','dept']\n",
							"schema2=['id','depname']\n",
							"df1=spark.createDataFrame(data1,schema1)\n",
							"df2=spark.createDataFrame(data2,schema2)\n",
							"df1.show()\n",
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"df1.join(df2,df1.dept==df2.id,'inner').show()\n",
							"df1.join(df2,df1.dept==df2.id,'left').show()\n",
							"df1.join(df2,df1.dept==df2.id,'right').show()\n",
							"df1.join(df2,df1.dept==df2.id,'leftsemi').show()\n",
							"df1.join(df2,df1.dept==df2.id,'leftanti').show()\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"data1=[(1,'kanit',2000,0),(2,'devi',3000,1),(3,'ani',1000,2)]\n",
							"schema1=['id','name','salary','mgrID']\n",
							"df1=spark.createDataFrame(data1,schema1)\n",
							"df1.show()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"df1.alias('emp').join(df1.alias('mgr'),col('emp.mgrID')==col('mgr.id'),'left')\\\n",
							".select(col('emp.name').alias('emp name'),col('mgr.name').alias('respective manager')).show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/partitionfunction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "da6f9b51-9e2a-4767-899e-0a2211d3ce75"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"data1=[(1,\"kanit\",'IT','M'),(2,\"Ani\",'HR','F'),(3,'Dev','IT','F')]\n",
							"schema1=[\"id\",\"name\",'Dept','Gender']\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\n",
							"df.write.parquet(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/wparquepartition',\\\n",
							"                partitionBy='Dept')"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"dfp=spark.read.parquet('abfss://kdlsfile@azurekdls.dfs.core.windows.net/wparquepartition/Dept=IT')\n",
							"display(dfp)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pivotunpivot')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f887cd88-d355-4e56-9613-30a7bd9efc76"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[(1,\"kanit\",'M',1000,'HR'),(2,\"Dev\",'F',2000,'IT'),(3,\"Ani\",'M',3000,'Account'),\\\n",
							"        (4,\"Ankit\",'M',4000,'Account'),(5,\"Hari\",'M',1000,'Account'),(6,\"meera\",'F',2000,'HR'),\\\n",
							"        (7,\"Anil\",'M',3000,'IT'),(8,\"tarun\",'M',4000,'IT'),(9,\"Ketan\",'M',4000,'IT')]\n",
							"schema1=[\"id\",\"name\",'gender','salary','department']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"df.groupBy(df.department).count().show()\n",
							"df.groupBy(df.department).pivot('gender').count().show()\n",
							"df.groupBy(df.department).pivot('gender',['M']).count().show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"data1=[('HR',8,5),('IT',6,3),('Account',1,2)]\n",
							"schema1=['department','male','female']\n",
							"df=spark.createDataFrame(data1,schema1)\n",
							"df.show()\n",
							"dfunpivot=df.select('department',expr(\"stack(2,'M',male,'F',female)as(gender,count)\"))\n",
							"dfunpivot.show()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rowandcolfunction')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a56a27ba-c708-48b1-825e-e0c6f2e2657c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"row1= Row(name='Kanit', age=25)\r\n",
							"row2= Row(name='Devyani', age=26)\r\n",
							"data1=[row1,row2]\r\n",
							"print(row1.name)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.createDataFrame(data1).show()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"person=Row('name','salary')\r\n",
							"person1=person('Kanit',1000)\r\n",
							"person2=person('Ankit',2000)\r\n",
							"data2=[person1,person2]\r\n",
							"print(person2.name)\r\n",
							"spark.createDataFrame(data2).show()"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"data1=[(1,\"kanit\",1000),(2,\"das\",2000)]\r\n",
							"schema1=[\"id\",\"name\",'salary']\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.withColumn('new sal',lit(5000)).show()"
						],
						"outputs": [],
						"execution_count": 38
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sample')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a2a47ca2-11c2-4404-be70-cd5c4d230aa4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"df=spark.range(start=1,end=101)\n",
							"df.show()\n",
							"#df1=df.sample(fraction=0.1).show()\n",
							"#df2=df.sample(fraction=0.1).show()\n",
							"df3=df.sample(fraction=0.1,seed=123).show()\n",
							"df4=df.sample(fraction=0.1,seed=123).show()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/show')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "06a6631e-8cbc-475a-9170-c253b51b115b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"data1 = [(1,\"Kanittttttttttttttttttttttttttttttttttttt\"),\r\n",
							"        (2,\"Devvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\"),\r\n",
							"        (3,\"Devvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvttt\"),]\r\n",
							"df=spark.createDataFrame(data=data1)\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/structuretype')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "95f80f66-08f1-4817-a4d7-51ec461b6f23"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"ron\",3000),\r\n",
							"        (2,\"don\",4000),\r\n",
							"        (3,\"son\",5000)]\r\n",
							"schema1=StructType(\\\r\n",
							"                    [StructField(name=\"ID\",dataType=IntegerType()),\\\r\n",
							"                    StructField(name=\"Name\",dataType=StringType()),\\\r\n",
							"                    StructField(name=\"Salary\",dataType=IntegerType())\\\r\n",
							"                    ])\r\n",
							"\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tojson')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "be7c491a-1235-4662-abb0-29b6e1b1091d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\n",
							"from pyspark.sql.functions import *\n",
							"from pyspark.sql.types import *\n",
							"data1=[('kanit',{\"hair\":\"black\",\"eye\":\"brown\"})]\n",
							"schema=['name','properties']\n",
							"df=spark.createDataFrame(data1,schema)\n",
							"df.show()\n",
							"df1=df.withColumn('propmap',to_json(df.properties))\n",
							"df1.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/unionanddistinct')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d67fa6dd-4cb0-464a-8256-aa678182ca2f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",'M',3000),(4,\"Ankit\",'M',4000)]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary']\r\n",
							"data2=[(1,\"Hari\",'M',1000),(2,\"meera\",'F',2000),(3,\"Anil\",'M',3000),(4,\"tarun\",'M',4000),\\\r\n",
							"        (4,\"Ankit\",'M',4000)]\r\n",
							"schema2=[\"id\",\"name\",'gender','salary']\r\n",
							"df1=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df1.show()\r\n",
							"df2=spark.createDataFrame(data=data2,schema=schema2)\r\n",
							"df2.show()\r\n",
							"uniondf=df1.union(df2)\r\n",
							"uniondf.show()\r\n",
							"uniondf.distinct().show()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",'M',3000),(4,\"Ankit\",'M',4000)]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary']\r\n",
							"data2=[(1,\"Hari\",'M',20),(2,\"meera\",'F',25),(3,\"Anil\",'M',30),(4,\"tarun\",'M',40),\\\r\n",
							"        (4,\"Ankit\",'M',45)]\r\n",
							"schema2=[\"id\",\"name\",'gender','age']\r\n",
							"df1=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df1.show()\r\n",
							"df2=spark.createDataFrame(data=data2,schema=schema2)\r\n",
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"uniondf=df1.unionByName(df2,allowMissingColumns=True).show()"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/whenotherwise')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "721da9eb-39de-4152-ab9c-10e7e54fa1a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"data1=[(1,\"kanit\",'M',1000),(2,\"Dev\",'F',2000),(3,\"Ani\",' ',3000)]\r\n",
							"schema1=[\"id\",\"name\",'gender','salary']\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df1=df.select(df.id,\\\r\n",
							"            df.name,\\\r\n",
							"            when(df.gender=='M','Male')\\\r\n",
							"            .when(df.gender==\"F\",'Female')\\\r\n",
							"            .otherwise('unknown').alias('Gender'))\r\n",
							"\r\n",
							"df1.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/withcloumn')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ca0ae0be-cf8c-4ad4-ba15-f2d4e4f97313"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"data1=[(1,\"ron\",\"3000\"),\r\n",
							"        (2,\"don\",\"4000\"),\r\n",
							"        (3,\"son\",\"5000\")]\r\n",
							"schema1=[\"id\",\"name\",\"salary\"]\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df1=df.withColumn('salary',col(\"salary\").cast(\"integer\"))\r\n",
							"df1.printSchema()"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2= df.withColumn(\"salary\",df.salary*3)\r\n",
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3= df.withColumn(\"Country\",lit('India'))\r\n",
							"df3.show()"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df4= df3.withColumn(\"new salary\",df.salary - 1000)\r\n",
							"df4.show()"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df5= df4.withColumnRenamed(\"new salary\",\"new salary amt\")\r\n",
							"df5.show()"
						],
						"outputs": [],
						"execution_count": 49
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/writecreatereadcsv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ab1d1050-5262-450f-9b64-8fa29819047d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"data1=[(1,\"kanit\"),(2,\"das\")]\r\n",
							"schema1=[\"id\",\"name\"]\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.csv(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writefile',header=True,mode='error')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.read.csv(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writefile',header=True))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.csv(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writefile',header=True,mode='append')"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.read.csv(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writefile',header=True))"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"display(spark.read.json(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/sample1.json',multiLine=True))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/writecreatereadjson')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8cd8f42d-06a0-4715-90f5-0ecd66a78f8b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"display(spark.read.json(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/sample1.json',multiLine=True))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"data1=[(1,\"kanit\"),(2,\"das\")]\r\n",
							"schema1=[\"id\",\"name\"]\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"display(df)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.json(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writejsonfile',mode='error')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.read.json(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writejsonfile'))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/writecreatereadparquet')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkdspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "202a2c95-651d-4b63-aaa9-a7dd30ca9dc5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/c853d123-c274-4d76-8290-e704c4a8dd26/resourceGroups/AzurePracticeRG/providers/Microsoft.Synapse/workspaces/azurekdsynapsespark/bigDataPools/sparkkdspool",
						"name": "sparkkdspool",
						"type": "Spark",
						"endpoint": "https://azurekdsynapsespark.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkdspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import *\r\n",
							"df=spark.read.parquet('abfss://kdlsfile@azurekdls.dfs.core.windows.net/mtcars.parquet')\r\n",
							"display(spark.read.parquet('abfss://kdlsfile@azurekdls.dfs.core.windows.net/mtcars.parquet'))\r\n",
							"print('number of rows is',df.count())\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"data1=[(1,\"kanit\"),(2,\"das\")]\r\n",
							"schema1=[\"id\",\"name\"]\r\n",
							"df=spark.createDataFrame(data=data1,schema=schema1)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.parquet(path='abfss://kdlsfile@azurekdls.dfs.core.windows.net/writeparquefile',mode='overwrite')"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfp=spark.read.parquet('abfss://kdlsfile@azurekdls.dfs.core.windows.net/writeparquefile')\r\n",
							"display(dfp)\r\n",
							"dfp.count()"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkkdspool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralus"
		}
	]
}